# dl research stuff


reading list:
- https://jalammar.github.io/
- https://wesmckinney.com/book/
- https://www.deeplearningbook.org/
- https://udlbook.github.io/udlbook/
- https://course.fast.ai/Lessons/lesson3.html
- https://jaykmody.com/
- https://lilianweng.github.io/posts/2021-09-25-train-large/
- https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/
- https://www.adept.ai/blog/act-1
- https://timdettmers.com/


- rnn: [https://colah.github.io/posts/2015-08-Understanding-LSTMs/](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
- stable d: [https://colab.research.google.com/drive/1dlgggNa5Mz8sEAGU0wFCHhGLFooW_pf1?usp=sharing](https://colab.research.google.com/drive/1dlgggNa5Mz8sEAGU0wFCHhGLFooW_pf1?usp=sharing)
- transformers:
    - [https://aman.ai/primers/ai/transformers/#one-hot-encoding](https://aman.ai/primers/ai/transformers/#one-hot-encoding)
    - [https://kipp.ly/blog/transformer-inference-arithmetic/](https://kipp.ly/blog/transformer-inference-arithmetic/)
    - https://github.com/dair-ai/Transformers-Recipe
    - [https://sebastianraschka.com/blog/2023/llm-reading-list.html](https://sebastianraschka.com/blog/2023/llm-reading-list.html)
    - [http://web.stanford.edu/class/cs224n/readings/cs224n-self-attention-transformers-2023_draft.pdf](http://web.stanford.edu/class/cs224n/readings/cs224n-self-attention-transformers-2023_draft.pdf)
    - [https://jaykmody.com/blog/gpt-from-scratch](https://jaykmody.com/blog/gpt-from-scratch)
- rl:
    - [https://arxiv.org/pdf/2301.01379.pdf](https://arxiv.org/pdf/2301.01379.pdf)

